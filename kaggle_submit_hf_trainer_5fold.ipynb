{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nKaggle 提交示例（Trainer + Dataset，5-fold head ensemble，timm ViT 单独加载）\\n\\n你需要做的只有两件事：\\n1) 把本文件顶部的全局路径变量改成你 Kaggle 实际的 /kaggle/input/... 目录\\n2) 确保 5 个 fold 的 best head 权重已按 A 格式存在：\\n   {HEAD_5FOLD_DIR}/fold0/model.safetensors\\n   ...\\n   {HEAD_5FOLD_DIR}/fold4/model.safetensors\\n\\n说明：\\n- 这些 head safetensors 来自你训练脚本 train_hf_trainer.py（HF Trainer 保存的 model.safetensors）\\n- 权重不包含 timm ViT，所以推理时会单独加载 ViT，并把 ViT 特征喂给 head\\n- 本脚本为 **FAST 双卡缓存推理**：两阶段（ViT 预计算缓存 -> head 推理），最后写出 submission.csv\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Kaggle 提交示例（Trainer + Dataset，5-fold head ensemble，timm ViT 单独加载）\n",
    "\n",
    "你需要做的只有两件事：\n",
    "1) 把本文件顶部的全局路径变量改成你 Kaggle 实际的 /kaggle/input/... 目录\n",
    "2) 确保 5 个 fold 的 best head 权重已按 A 格式存在：\n",
    "   {HEAD_5FOLD_DIR}/fold0/model.safetensors\n",
    "   ...\n",
    "   {HEAD_5FOLD_DIR}/fold4/model.safetensors\n",
    "\n",
    "说明：\n",
    "- 这些 head safetensors 来自你训练脚本 train_hf_trainer.py（HF Trainer 保存的 model.safetensors）\n",
    "- 权重不包含 timm ViT，所以推理时会单独加载 ViT，并把 ViT 特征喂给 head\n",
    "- 本脚本为 **FAST 双卡缓存推理**：两阶段（ViT 预计算缓存 -> head 推理），最后写出 submission.csv\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import gc\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm\n",
    "from safetensors.torch import load_file as safetensors_load_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 需要你按 Kaggle 实际情况替换的全局路径变量（我先假设一套）\n",
    "# =============================================================================\n",
    "KAGGLE_DATA_DIR = Path(\"/home/ecs-user/code/happen/kaggle-csrio\")  # 里面有 csiro-biomass/test.csv + test/...\n",
    "KAGGLE_VIT_DIR = Path(\"/home/ecs-user/code/happen/kaggle-csrio/timm/vit_7b_patch16_dinov3.lvd1689m\")  # 可选：本地 vit 权重目录（里面有 model.safetensors）\n",
    "KAGGLE_HEAD_5FOLD_DIR = Path(\"/home/ecs-user/code/happen/kaggle-csrio/runs/hf_trainer_A\")  # A 格式目录\n",
    "\n",
    "# 输出目录（Kaggle 工作目录可写）\n",
    "OUTPUT_DIR = Path(\"./kaggle_out\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 你训练时的相对数据目录结构（与仓库一致）\n",
    "TEST_CSV = KAGGLE_DATA_DIR / \"csiro-biomass\" / \"test.csv\"\n",
    "IMAGE_ROOT = KAGGLE_DATA_DIR / \"csiro-biomass\"\n",
    "SAMPLE_SUB = KAGGLE_DATA_DIR / \"csiro-biomass\" / \"sample_submission.csv\"\n",
    "\n",
    "# 5-fold\n",
    "FOLDS = (0, 1, 2, 3, 4)\n",
    "\n",
    "# 单卡建议：batch 尽量小（ViT-7B 很吃显存）\n",
    "# =============================================================================\n",
    "# Fast 推理（两阶段：ViT 预计算缓存 -> head 推理），双进程/双卡并行\n",
    "# =============================================================================\n",
    "GPU_IDS = (0, 1)  # 物理 GPU id（Kaggle 双卡常见是 0/1）\n",
    "VIT_BATCH_SIZE = 2  # ViT 预计算阶段 batch（太大容易 OOM）\n",
    "HEAD_BATCH_SIZE = 32  # head 推理阶段 batch（通常可以大很多）\n",
    "CACHE_DIR = OUTPUT_DIR / \"vit_cache\"\n",
    "# ViT 预处理模式：\n",
    "# - \"exact_timm\": 恢复原来的 backbone_transform 路径（timm.create_transform），最接近原版结果，但更慢\n",
    "# - \"fast_gpu\": 纯 torch GPU resize+normalize（更快，但会产生可见差异）\n",
    "VIT_PREPROCESS_MODE = \"exact_timm\"  # \"exact_timm\" or \"fast_gpu\"\n",
    "# 缓存精度：\n",
    "# - \"fp16\": 更快/更省磁盘，但会引入量化误差（submission 可能与非缓存版略有差异）\n",
    "# - \"fp32\": 更接近“直接推理不落盘”的结果，但更占磁盘/IO\n",
    "CACHE_DTYPE = \"fp32\"  # \"fp16\" or \"fp32\"\n",
    "\n",
    "# Head 推理精度策略（为了更贴近 Trainer 的 AMP 行为）\n",
    "# - head 权重保持 fp32，forward 用 autocast（很多算子会自动选择更稳定的内部精度）\n",
    "HEAD_WEIGHT_DTYPE = \"fp32\"  # \"fp32\"（推荐）或 \"fp16\"/\"bf16\"\n",
    "USE_AUTOCAST_FOR_HEAD = True\n",
    "\n",
    "# ViT 推理时是否也启用 autocast（进一步贴近 Trainer/AMP 行为）\n",
    "USE_AUTOCAST_FOR_VIT = True\n",
    "\n",
    "# Notebook 里 multiprocessing(spawn) 经常会因为函数无法从 __main__ 导入而报：\n",
    "# \"Can't get attribute '_vit_precompute_worker' on <module '__main__' ...>\"\n",
    "# 为了在 notebook 里也能双卡并行，这里提供线程后端（同一进程两个线程分别绑定两张 GPU）。\n",
    "# - \"process\": 用多进程（更隔离，Kaggle Script 场景推荐）\n",
    "# - \"thread\": 用多线程（Notebook 场景推荐，避免 spawn pickle/import 问题）\n",
    "PARALLEL_BACKEND = \"thread\"  # \"thread\" or \"process\"\n",
    "VIT_INIT_CPU_ONCE_CLONE_TO_GPU = True  # 仅 thread 后端生效：CPU 只加载 1 次 ViT，然后复制到两张 GPU\n",
    "\n",
    "\n",
    "def _in_notebook() -> bool:\n",
    "    return \"ipykernel\" in sys.modules or \"IPython\" in sys.modules\n",
    "\n",
    "\n",
    "def _run_parallel_jobs(jobs: list[tuple[callable, dict]], *, backend: str) -> None:\n",
    "    \"\"\"\n",
    "    jobs: [(fn, kwargs), ...]\n",
    "    backend:\n",
    "      - \"process\": spawn 子进程执行（要求 fn 可 picklable 且在可 import 的模块顶层定义）\n",
    "      - \"thread\": 线程执行（Notebook 友好；同进程共享内存，需注意不要写同一段 memmap）\n",
    "    \"\"\"\n",
    "    if backend == \"process\":\n",
    "        ctx = mp.get_context(\"spawn\")\n",
    "        procs = []\n",
    "        for fn, kwargs in jobs:\n",
    "            p = ctx.Process(target=fn, kwargs=kwargs)\n",
    "            p.start()\n",
    "            procs.append(p)\n",
    "        for p in procs:\n",
    "            p.join()\n",
    "            if p.exitcode != 0:\n",
    "                raise RuntimeError(f\"子进程异常退出：exitcode={p.exitcode}\")\n",
    "        return\n",
    "\n",
    "    # thread backend\n",
    "    with ThreadPoolExecutor(max_workers=len(jobs)) as ex:\n",
    "        futs = [ex.submit(fn, **kwargs) for fn, kwargs in jobs]\n",
    "        for fut in as_completed(futs):\n",
    "            # re-raise any exception\n",
    "            fut.result()\n",
    "\n",
    "\n",
    "# thread 后端下的共享 ViT（每张 GPU 一份），避免每个线程各自从磁盘/Hub 加载\n",
    "_VIT_THREAD_MODELS: dict[int, tuple[nn.Module, object]] = {}\n",
    "\n",
    "\n",
    "def _create_vit_model_uninitialized_on_device(device: torch.device) -> nn.Module:\n",
    "    \"\"\"\n",
    "    在指定 device 上创建与 ViT-7B 同结构的模型，但不从磁盘/Hub 加载权重（pretrained=False）。\n",
    "    通过 torch.set_default_device 尽量避免在 CPU 上分配巨型参数（对 7B 很关键）。\n",
    "    \"\"\"\n",
    "    # torch.set_default_device 是全局的，尽量短时间使用\n",
    "    torch.set_default_device(str(device))\n",
    "    try:\n",
    "        if CFG.vit_load_from_hf_hub:\n",
    "            m = timm.create_model(\n",
    "                CFG.vit_hf_hub_id,\n",
    "                pretrained=False,\n",
    "                num_classes=0,\n",
    "                global_pool=\"avg\",\n",
    "            )\n",
    "        else:\n",
    "            m = timm.create_model(\n",
    "                CFG.vit_name,\n",
    "                pretrained=False,\n",
    "                num_classes=0,\n",
    "                global_pool=\"avg\",\n",
    "            )\n",
    "    finally:\n",
    "        torch.set_default_device(\"cpu\")\n",
    "    return m\n",
    "\n",
    "\n",
    "def _init_vit_models_for_thread_backend(gpu_ids: Tuple[int, int], dtype: torch.dtype) -> None:\n",
    "    \"\"\"\n",
    "    CPU 上加载一次预训练 ViT，然后复制到多个 GPU（每个 GPU 一份），供 thread 后端并行使用。\n",
    "    \"\"\"\n",
    "    global _VIT_THREAD_MODELS\n",
    "    if _VIT_THREAD_MODELS:\n",
    "        return\n",
    "\n",
    "    # 1) CPU 加载一次（占用一次 CPU 内存）\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    if CFG.vit_load_from_hf_hub:\n",
    "        cpu_model = timm.create_model(\n",
    "            CFG.vit_hf_hub_id,\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            global_pool=\"avg\",\n",
    "        ).to(cpu_device)\n",
    "    else:\n",
    "        if not CFG.vit_checkpoint.exists():\n",
    "            raise FileNotFoundError(f\"ViT checkpoint 不存在: {CFG.vit_checkpoint}\")\n",
    "        cpu_model = timm.create_model(\n",
    "            CFG.vit_name,\n",
    "            pretrained=False,\n",
    "            num_classes=0,\n",
    "            global_pool=\"avg\",\n",
    "            checkpoint_path=str(CFG.vit_checkpoint),\n",
    "        ).to(cpu_device)\n",
    "\n",
    "    cpu_model.eval()\n",
    "    data_config = timm.data.resolve_model_data_config(cpu_model)\n",
    "    backbone_transform = timm.data.create_transform(**data_config, is_training=False)\n",
    "    state = cpu_model.state_dict()  # 引用 CPU 权重张量（不会额外复制）\n",
    "\n",
    "    # 2) 逐 GPU 创建模型并加载权重（CPU -> GPU copy）\n",
    "    for dev_id in gpu_ids:\n",
    "        torch.cuda.set_device(dev_id)\n",
    "        dev = torch.device(f\"cuda:{dev_id}\")\n",
    "        m = _create_vit_model_uninitialized_on_device(dev)\n",
    "        missing, unexpected = m.load_state_dict(state, strict=False)\n",
    "        if missing or unexpected:\n",
    "            raise RuntimeError(f\"ViT load_state_dict 不匹配：missing={missing[:5]} unexpected={unexpected[:5]}\")\n",
    "        m.eval()\n",
    "        m = m.to(device=dev, dtype=dtype)\n",
    "        _VIT_THREAD_MODELS[dev_id] = (m, backbone_transform)\n",
    "\n",
    "    # 3) 释放 CPU 模型与 state（此时 GPU 已各自持有权重）\n",
    "    del cpu_model\n",
    "    del state\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 目标列定义（需与训练一致）\n",
    "# =============================================================================\n",
    "TARGET_COLS = (\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 训练/推理配置（需要与 train_hf_trainer.py 的 HFConfig 对齐）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 如果你训练时改过 small_grid/big_grid/pyramid_dims/token_embed_dim 等结构超参，\n",
    "# 请在这里同步修改，否则 load_state_dict 会报错（结构不匹配）。\n",
    "# =============================================================================\n",
    "class InferCFG:\n",
    "    # ViT 加载方式：\n",
    "    # - 推荐：Kaggle 开网时用 hf_hub 直接拉取预训练权重（你给的方式）\n",
    "    # - 备选：不开网/想固定版本时，用本地 checkpoint_path\n",
    "    vit_load_from_hf_hub: bool = False\n",
    "    vit_hf_hub_id: str = \"hf_hub:timm/vit_7b_patch16_dinov3.lvd1689m\"\n",
    "    vit_name: str = \"vit_7b_patch16_dinov3.lvd1689m\"  # 本地 ckpt 方式需要\n",
    "    vit_checkpoint: Path = KAGGLE_VIT_DIR / \"model.safetensors\"  # 本地 ckpt 方式需要\n",
    "    vit_feat_dim: int = 4096\n",
    "    token_embed_dim: int = 1024\n",
    "\n",
    "    # tile grid（与你 train_hf_trainer.py 一致）\n",
    "    small_grid: Tuple[int, int] = (2, 4)\n",
    "    big_grid: Tuple[int, int] = (1, 2)\n",
    "\n",
    "    # head 结构（与你 train_hf_trainer.py 一致）\n",
    "    dropout: float = 0.1\n",
    "    hidden_ratio: float = 0.35\n",
    "    aux_head: bool = True\n",
    "    pyramid_dims: Tuple[int, int, int] = (768, 1024, 1280)\n",
    "    mamba_depth: int = 3\n",
    "    mamba_kernel: int = 5\n",
    "    mobilevit_heads: int = 4\n",
    "    mobilevit_depth: int = 2\n",
    "    sra_heads: int = 8\n",
    "    sra_ratio: int = 2\n",
    "    cross_heads: int = 8\n",
    "    cross_layers: int = 2\n",
    "    t2t_depth: int = 2\n",
    "\n",
    "\n",
    "CFG = InferCFG()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Head 结构（为保证脚本独立运行，这里直接内置一份与 train_hf_trainer.py 对齐的实现）\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class HFConfig:\n",
    "    dropout: float = 0.1\n",
    "    hidden_ratio: float = 0.35\n",
    "    aux_head: bool = True\n",
    "\n",
    "    small_grid: Tuple[int, int] = (2, 4)\n",
    "    big_grid: Tuple[int, int] = (1, 2)\n",
    "    pyramid_dims: Tuple[int, int, int] = (768, 1024, 1280)\n",
    "    mamba_depth: int = 3\n",
    "    mamba_kernel: int = 5\n",
    "    mobilevit_heads: int = 4\n",
    "    mobilevit_depth: int = 2\n",
    "    sra_heads: int = 8\n",
    "    sra_ratio: int = 2\n",
    "    cross_heads: int = 8\n",
    "    cross_layers: int = 2\n",
    "    t2t_depth: int = 2\n",
    "\n",
    "    vit_name: str = \"vit_7b_patch16_dinov3.lvd1689m\"\n",
    "    vit_checkpoint: Path = Path(\"unused\")\n",
    "    vit_feat_dim: int = 4096\n",
    "    token_embed_dim: int = 1024\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, mlp_ratio: float = 4.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        hid = int(dim * mlp_ratio)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hid),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hid, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, dim: int, heads: int = 8, dropout: float = 0.0, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ff = FeedForward(dim, mlp_ratio=mlp_ratio, dropout=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.norm1(x)\n",
    "        attn_out, _ = self.attn(h, h, h, need_weights=False)\n",
    "        x = x + attn_out\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, dim: int, heads: int = 4, depth: int = 2, patch: Tuple[int, int] = (2, 2), dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.local = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, 3, padding=1, groups=dim),\n",
    "            nn.Conv2d(dim, dim, 1),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.patch = patch\n",
    "        self.transformer = nn.ModuleList(\n",
    "            [AttentionBlock(dim, heads=heads, dropout=dropout, mlp_ratio=2.0) for _ in range(depth)]\n",
    "        )\n",
    "        self.fuse = nn.Conv2d(dim * 2, dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        local_feat = self.local(x)\n",
    "        bsz, ch, h, w = local_feat.shape\n",
    "        ph, pw = self.patch\n",
    "        new_h = math.ceil(h / ph) * ph\n",
    "        new_w = math.ceil(w / pw) * pw\n",
    "        if new_h != h or new_w != w:\n",
    "            local_feat = F.interpolate(local_feat, size=(new_h, new_w), mode=\"bilinear\", align_corners=False)\n",
    "            h, w = new_h, new_w\n",
    "\n",
    "        tokens = local_feat.unfold(2, ph, ph).unfold(3, pw, pw)  # B,C,nh,nw,ph,pw\n",
    "        tokens = tokens.contiguous().view(bsz, ch, -1, ph, pw)\n",
    "        tokens = tokens.permute(0, 2, 3, 4, 1).reshape(bsz, -1, ch)\n",
    "\n",
    "        for blk in self.transformer:\n",
    "            tokens = blk(tokens)\n",
    "\n",
    "        feat = tokens.view(bsz, -1, ph * pw, ch).permute(0, 3, 1, 2)\n",
    "        nh = h // ph\n",
    "        nw = w // pw\n",
    "        feat = feat.view(bsz, ch, nh, nw, ph, pw).permute(0, 1, 2, 4, 3, 5)\n",
    "        feat = feat.reshape(bsz, ch, h, w)\n",
    "\n",
    "        if feat.shape[-2:] != x.shape[-2:]:\n",
    "            feat = F.interpolate(feat, size=x.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        out = self.fuse(torch.cat([x, feat], dim=1))\n",
    "        return out\n",
    "\n",
    "\n",
    "class SpatialReductionAttention(nn.Module):\n",
    "    def __init__(self, dim: int, heads: int = 8, sr_ratio: int = 2, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.kv = nn.Linear(dim, dim * 2)\n",
    "        self.sr_ratio = sr_ratio\n",
    "        if sr_ratio > 1:\n",
    "            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "        else:\n",
    "            self.sr = None\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hw: Tuple[int, int]) -> torch.Tensor:\n",
    "        bsz, n, ch = x.shape\n",
    "        q = self.q(x).reshape(bsz, n, self.heads, ch // self.heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        if self.sr is not None:\n",
    "            h, w = hw\n",
    "            feat = x.transpose(1, 2).reshape(bsz, ch, h, w)\n",
    "            feat = self.sr(feat)\n",
    "            feat = feat.reshape(bsz, ch, -1).transpose(1, 2)\n",
    "            feat = self.norm(feat)\n",
    "        else:\n",
    "            feat = x\n",
    "\n",
    "        kv = self.kv(feat)\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "        k = k.reshape(bsz, -1, self.heads, ch // self.heads).permute(0, 2, 3, 1)\n",
    "        v = v.reshape(bsz, -1, self.heads, ch // self.heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        attn = torch.matmul(q, k) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.drop(attn)\n",
    "        out = torch.matmul(attn, v).permute(0, 2, 1, 3).reshape(bsz, n, ch)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PVTBlock(nn.Module):\n",
    "    def __init__(self, dim: int, heads: int = 8, sr_ratio: int = 2, dropout: float = 0.0, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.sra = SpatialReductionAttention(dim, heads=heads, sr_ratio=sr_ratio, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ff = FeedForward(dim, mlp_ratio=mlp_ratio, dropout=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hw: Tuple[int, int]) -> torch.Tensor:\n",
    "        x = x + self.sra(self.norm1(x), hw)\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class LocalMambaBlock(nn.Module):\n",
    "    def __init__(self, dim: int, kernel_size: int = 5, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.dwconv = nn.Conv1d(dim, dim, kernel_size=kernel_size, padding=kernel_size // 2, groups=dim)\n",
    "        self.gate = nn.Linear(dim, dim)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        shortcut = x\n",
    "        x = self.norm(x)\n",
    "        g = torch.sigmoid(self.gate(x))\n",
    "        x = (x * g).transpose(1, 2)\n",
    "        x = self.dwconv(x).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        x = self.drop(x)\n",
    "        return shortcut + x\n",
    "\n",
    "\n",
    "class T2TRetokenizer(nn.Module):\n",
    "    def __init__(self, dim: int, depth: int = 2, heads: int = 4, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [AttentionBlock(dim, heads=heads, dropout=dropout, mlp_ratio=2.0) for _ in range(depth)]\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, grid_hw: Tuple[int, int]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        bsz, _, ch = tokens.shape\n",
    "        h, w = grid_hw\n",
    "        feat_map = tokens.transpose(1, 2).reshape(bsz, ch, h, w)\n",
    "        seq = feat_map.flatten(2).transpose(1, 2)\n",
    "        for blk in self.blocks:\n",
    "            seq = blk(seq)\n",
    "        seq_map = seq.transpose(1, 2).reshape(bsz, ch, h, w)\n",
    "        pooled = F.adaptive_avg_pool2d(seq_map, (2, 2))\n",
    "        retokens = pooled.flatten(2).transpose(1, 2)\n",
    "        return retokens, seq_map\n",
    "\n",
    "\n",
    "class CrossScaleFusion(nn.Module):\n",
    "    def __init__(self, dim: int, heads: int = 6, dropout: float = 0.0, layers: int = 2):\n",
    "        super().__init__()\n",
    "        self.layers_s = nn.ModuleList(\n",
    "            [AttentionBlock(dim, heads=heads, dropout=dropout, mlp_ratio=2.0) for _ in range(layers)]\n",
    "        )\n",
    "        self.layers_b = nn.ModuleList(\n",
    "            [AttentionBlock(dim, heads=heads, dropout=dropout, mlp_ratio=2.0) for _ in range(layers)]\n",
    "        )\n",
    "        self.cross_s = nn.ModuleList(\n",
    "            [nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True, kdim=dim, vdim=dim) for _ in range(layers)]\n",
    "        )\n",
    "        self.cross_b = nn.ModuleList(\n",
    "            [nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True, kdim=dim, vdim=dim) for _ in range(layers)]\n",
    "        )\n",
    "        self.norm_s = nn.LayerNorm(dim)\n",
    "        self.norm_b = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, tok_s: torch.Tensor, tok_b: torch.Tensor) -> torch.Tensor:\n",
    "        bsz, _, ch = tok_s.shape\n",
    "        cls_s = tok_s.new_zeros(bsz, 1, ch)\n",
    "        cls_b = tok_b.new_zeros(bsz, 1, ch)\n",
    "        tok_s = torch.cat([cls_s, tok_s], dim=1)\n",
    "        tok_b = torch.cat([cls_b, tok_b], dim=1)\n",
    "\n",
    "        for ls, lb, cs, cb in zip(self.layers_s, self.layers_b, self.cross_s, self.cross_b):\n",
    "            tok_s = ls(tok_s)\n",
    "            tok_b = lb(tok_b)\n",
    "            q_s = self.norm_s(tok_s[:, :1])\n",
    "            q_b = self.norm_b(tok_b[:, :1])\n",
    "            cls_s_upd, _ = cs(q_s, torch.cat([tok_b, q_b], dim=1), torch.cat([tok_b, q_b], dim=1), need_weights=False)\n",
    "            cls_b_upd, _ = cb(q_b, torch.cat([tok_s, q_s], dim=1), torch.cat([tok_s, q_s], dim=1), need_weights=False)\n",
    "            tok_s = torch.cat([tok_s[:, :1] + cls_s_upd, tok_s[:, 1:]], dim=1)\n",
    "            tok_b = torch.cat([tok_b[:, :1] + cls_b_upd, tok_b[:, 1:]], dim=1)\n",
    "\n",
    "        tokens = torch.cat([tok_s[:, :1], tok_b[:, :1], tok_s[:, 1:], tok_b[:, 1:]], dim=1)\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class PyramidMixer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in: int,\n",
    "        dims: Tuple[int, int, int],\n",
    "        mobilevit_heads: int = 4,\n",
    "        mobilevit_depth: int = 2,\n",
    "        sra_heads: int = 6,\n",
    "        sra_ratio: int = 2,\n",
    "        mamba_depth: int = 3,\n",
    "        mamba_kernel: int = 5,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        c1, c2, c3 = dims\n",
    "        self.proj1 = nn.Linear(dim_in, c1)\n",
    "        self.mobilevit = MobileViTBlock(c1, heads=mobilevit_heads, depth=mobilevit_depth, dropout=dropout)\n",
    "        self.proj2 = nn.Linear(c1, c2)\n",
    "        self.pvt = PVTBlock(c2, heads=sra_heads, sr_ratio=sra_ratio, dropout=dropout, mlp_ratio=3.0)\n",
    "        self.mamba_local = LocalMambaBlock(c2, kernel_size=mamba_kernel, dropout=dropout)\n",
    "        self.proj3 = nn.Linear(c2, c3)\n",
    "        self.mamba_global = nn.ModuleList([LocalMambaBlock(c3, kernel_size=mamba_kernel, dropout=dropout) for _ in range(mamba_depth)])\n",
    "        self.final_attn = AttentionBlock(c3, heads=min(8, c3 // 64 + 1), dropout=dropout, mlp_ratio=2.0)\n",
    "\n",
    "    def _tokens_to_map(self, tokens: torch.Tensor, target_hw: Tuple[int, int]) -> torch.Tensor:\n",
    "        bsz, n, ch = tokens.shape\n",
    "        h, w = target_hw\n",
    "        need = h * w\n",
    "        if n < need:\n",
    "            pad = tokens.new_zeros(bsz, need - n, ch)\n",
    "            tokens = torch.cat([tokens, pad], dim=1)\n",
    "        tokens = tokens[:, :need, :]\n",
    "        feat_map = tokens.transpose(1, 2).reshape(bsz, ch, h, w)\n",
    "        return feat_map\n",
    "\n",
    "    @staticmethod\n",
    "    def _fit_hw(n_tokens: int) -> Tuple[int, int]:\n",
    "        h = int(math.sqrt(n_tokens))\n",
    "        w = h\n",
    "        while h * w < n_tokens:\n",
    "            w += 1\n",
    "            if h * w < n_tokens:\n",
    "                h += 1\n",
    "        return h, w\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        bsz, n, _ = tokens.shape\n",
    "        map_hw = (3, 4)\n",
    "        t1 = self.proj1(tokens)\n",
    "        m1 = self._tokens_to_map(t1, map_hw)\n",
    "        m1 = self.mobilevit(m1)\n",
    "        t1_out = m1.flatten(2).transpose(1, 2)[:, :n]\n",
    "\n",
    "        t2 = self.proj2(t1_out)\n",
    "        new_len = max(4, n // 2)\n",
    "        t2 = t2[:, :new_len] + F.adaptive_avg_pool1d(t2.transpose(1, 2), new_len).transpose(1, 2)\n",
    "        hw2 = self._fit_hw(t2.size(1))\n",
    "        if t2.size(1) < hw2[0] * hw2[1]:\n",
    "            pad = t2.new_zeros(bsz, hw2[0] * hw2[1] - t2.size(1), t2.size(2))\n",
    "            t2 = torch.cat([t2, pad], dim=1)\n",
    "        t2 = self.pvt(t2, hw2)\n",
    "        t2 = self.mamba_local(t2)\n",
    "\n",
    "        t3 = self.proj3(t2)\n",
    "        pooled = torch.stack([t3.mean(dim=1), t3.max(dim=1).values], dim=1)\n",
    "        t3 = pooled\n",
    "        for blk in self.mamba_global:\n",
    "            t3 = blk(t3)\n",
    "        t3 = self.final_attn(t3)\n",
    "        global_feat = t3.mean(dim=1)\n",
    "        return global_feat, {\"stage1_map\": m1, \"stage2_tokens\": t2, \"stage3_tokens\": t3}\n",
    "\n",
    "\n",
    "class CrossPVT_T2T_MambaHead(nn.Module):\n",
    "    def __init__(self, cfg: HFConfig):\n",
    "        super().__init__()\n",
    "        self.feat_dim = cfg.vit_feat_dim\n",
    "        self.token_embed = nn.Linear(cfg.vit_feat_dim, cfg.token_embed_dim)\n",
    "        self.t2t = T2TRetokenizer(cfg.token_embed_dim, depth=cfg.t2t_depth, heads=cfg.cross_heads, dropout=cfg.dropout)\n",
    "        self.cross = CrossScaleFusion(cfg.token_embed_dim, heads=cfg.cross_heads, dropout=cfg.dropout, layers=cfg.cross_layers)\n",
    "        self.pyramid = PyramidMixer(\n",
    "            dim_in=cfg.token_embed_dim,\n",
    "            dims=cfg.pyramid_dims,\n",
    "            mobilevit_heads=cfg.mobilevit_heads,\n",
    "            mobilevit_depth=cfg.mobilevit_depth,\n",
    "            sra_heads=cfg.sra_heads,\n",
    "            sra_ratio=cfg.sra_ratio,\n",
    "            mamba_depth=cfg.mamba_depth,\n",
    "            mamba_kernel=cfg.mamba_kernel,\n",
    "            dropout=cfg.dropout,\n",
    "        )\n",
    "\n",
    "        combined = cfg.pyramid_dims[-1]\n",
    "        hidden = max(32, int(combined * cfg.hidden_ratio))\n",
    "\n",
    "        def head() -> nn.Sequential:\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(combined, hidden),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(cfg.dropout),\n",
    "                nn.Linear(hidden, 1),\n",
    "            )\n",
    "\n",
    "        self.head_green = head()\n",
    "        self.head_clover = head()\n",
    "        self.head_dead = head()\n",
    "        # 训练脚本里存在但未必参与 loss 的 score_head：为了 load_state_dict 完全一致，这里保留\n",
    "        self.score_head = nn.Sequential(nn.LayerNorm(combined), nn.Linear(combined, 1))\n",
    "        self.aux_head = (\n",
    "            nn.Sequential(nn.LayerNorm(cfg.pyramid_dims[1]), nn.Linear(cfg.pyramid_dims[1], len(TARGET_COLS))) if cfg.aux_head else None\n",
    "        )\n",
    "        self.softplus = nn.Softplus(beta=1.0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tokens_small: torch.Tensor,\n",
    "        tokens_big: torch.Tensor,\n",
    "        small_grid: Tuple[int, int],\n",
    "        return_features: bool = False,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        tokens_small = self.token_embed(tokens_small)\n",
    "        tokens_big = self.token_embed(tokens_big)\n",
    "\n",
    "        t2, stage1_map = self.t2t(tokens_small, small_grid)\n",
    "        fused = self.cross(t2, tokens_big)\n",
    "        feat, feat_maps = self.pyramid(fused)\n",
    "        feat_maps[\"stage1_map\"] = stage1_map\n",
    "\n",
    "        green_pos = self.softplus(self.head_green(feat))\n",
    "        clover_pos = self.softplus(self.head_clover(feat))\n",
    "        dead_pos = self.softplus(self.head_dead(feat))\n",
    "\n",
    "        out: Dict[str, torch.Tensor] = {\"green\": green_pos, \"dead\": dead_pos, \"clover\": clover_pos, \"score_feat\": feat}\n",
    "        if self.aux_head is not None:\n",
    "            aux_tokens = feat_maps[\"stage2_tokens\"]\n",
    "            aux_pred = self.softplus(self.aux_head(aux_tokens.mean(dim=1)))\n",
    "            out[\"aux\"] = aux_pred\n",
    "        if return_features:\n",
    "            out[\"feature_maps\"] = {\"stage1\": feat_maps.get(\"stage1_map\"), \"stage3\": feat_maps.get(\"stage3_tokens\")}\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# timm ViT：单独加载 + 冻结\n",
    "# =============================================================================\n",
    "def pick_infer_dtype() -> torch.dtype:\n",
    "    # Kaggle 常见 T4: bf16 不支持，优先 fp16；没有 cuda 就 fp32\n",
    "    if not torch.cuda.is_available():\n",
    "        return torch.float32\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        return torch.bfloat16\n",
    "    return torch.float16\n",
    "\n",
    "\n",
    "def build_vit_backbone(\n",
    "    *,\n",
    "    vit_name: str,\n",
    "    vit_ckpt: Path,\n",
    "    device: torch.device,\n",
    "    dtype: torch.dtype,\n",
    "):\n",
    "    # 优先：从 HuggingFace Hub 直接拉取 timm 模型（需要 Kaggle 开网）\n",
    "    if CFG.vit_load_from_hf_hub:\n",
    "        model = timm.create_model(\n",
    "            CFG.vit_hf_hub_id,\n",
    "            pretrained=True,\n",
    "            num_classes=0,\n",
    "            global_pool=\"avg\",\n",
    "        )\n",
    "    else:\n",
    "        if not vit_ckpt.exists():\n",
    "            raise FileNotFoundError(f\"ViT checkpoint 不存在: {vit_ckpt}\")\n",
    "        model = timm.create_model(\n",
    "            vit_name,\n",
    "            pretrained=False,\n",
    "            num_classes=0,\n",
    "            global_pool=\"avg\",\n",
    "            checkpoint_path=str(vit_ckpt),\n",
    "        )\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    model.eval()\n",
    "    model = model.to(device=device, dtype=dtype)\n",
    "\n",
    "    data_config = timm.data.resolve_model_data_config(model)\n",
    "    transform = timm.data.create_transform(**data_config, is_training=False)\n",
    "    return model, transform\n",
    "\n",
    "\n",
    "def _vit_mean_std() -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # 来自 timm/vit_7b_patch16_dinov3.lvd1689m/config.json\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).view(1, 3, 1, 1)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def _resize_norm_batch(x: torch.Tensor, *, out_hw: Tuple[int, int], dtype: torch.dtype) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: [B,3,H,W]，值域假设在 [0,1]\n",
    "    输出：resize 到 out_hw + timm mean/std normalize\n",
    "    \"\"\"\n",
    "    # timm/torchvision 的 Resize 通常会做 antialias；这里打开 antialias 以尽量贴近 timm 行为\n",
    "    x = F.interpolate(x, size=out_hw, mode=\"bicubic\", align_corners=False, antialias=True)\n",
    "    mean, std = _vit_mean_std()\n",
    "    mean = mean.to(device=x.device, dtype=x.dtype)\n",
    "    std = std.to(device=x.device, dtype=x.dtype)\n",
    "    x = (x - mean) / std\n",
    "    return x.to(dtype)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_tiles_fast(\n",
    "    images: torch.Tensor,\n",
    "    backbone: nn.Module,\n",
    "    grid: Tuple[int, int],\n",
    "    *,\n",
    "    dtype: torch.dtype,\n",
    "    vit_input_hw: Tuple[int, int] = (256, 256),\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    更快的 tile 编码：\n",
    "    - 不走 timm.create_transform（避免逐 tile python 循环）\n",
    "    - 直接在 GPU 上 resize + normalize\n",
    "    \"\"\"\n",
    "    bsz, ch, h, w = images.shape\n",
    "    r, c = grid\n",
    "    hs = torch.linspace(0, h, steps=r + 1, device=images.device).round().long()\n",
    "    ws = torch.linspace(0, w, steps=c + 1, device=images.device).round().long()\n",
    "\n",
    "    tiles: List[torch.Tensor] = []\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            rs, re = hs[i].item(), hs[i + 1].item()\n",
    "            cs, ce = ws[j].item(), ws[j + 1].item()\n",
    "            tiles.append(images[:, :, rs:re, cs:ce])\n",
    "    tiles = torch.stack(tiles, dim=1)  # [B,T,C,Ht,Wt]\n",
    "    flat = tiles.reshape(-1, ch, tiles.shape[-2], tiles.shape[-1])  # [B*T,C,Ht,Wt]\n",
    "    flat = _resize_norm_batch(flat, out_hw=vit_input_hw, dtype=dtype)\n",
    "    if USE_AUTOCAST_FOR_VIT and flat.is_cuda:\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=dtype):\n",
    "            feats = backbone(flat)  # [B*T, feat]\n",
    "    else:\n",
    "        feats = backbone(flat)  # [B*T, feat]\n",
    "    feats = feats.view(bsz, -1, feats.shape[-1])  # [B, T, feat]\n",
    "    return feats\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_tiles_exact_timm_transform(\n",
    "    images: torch.Tensor,\n",
    "    backbone: nn.Module,\n",
    "    backbone_transform,\n",
    "    grid: Tuple[int, int],\n",
    "    *,\n",
    "    dtype: torch.dtype,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    恢复原版路径：逐 tile 调用 timm.create_transform（backbone_transform），以最大程度复现原先输出。\n",
    "    注意：该路径会比 fast_gpu 慢（存在 python for 循环）。\n",
    "    \"\"\"\n",
    "    bsz, ch, h, w = images.shape\n",
    "    r, c = grid\n",
    "    hs = torch.linspace(0, h, steps=r + 1).round().long()\n",
    "    ws = torch.linspace(0, w, steps=c + 1).round().long()\n",
    "\n",
    "    tiles: List[torch.Tensor] = []\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            rs, re = hs[i].item(), hs[i + 1].item()\n",
    "            cs, ce = ws[j].item(), ws[j + 1].item()\n",
    "            tiles.append(images[:, :, rs:re, cs:ce])\n",
    "    tiles = torch.stack(tiles, dim=1)  # [B,T,C,Ht,Wt]\n",
    "    flat = tiles.view(-1, ch, tiles.shape[-2], tiles.shape[-1])  # [B*T,C,Ht,Wt]\n",
    "\n",
    "    # 与原始 train_hf_trainer.py 的做法一致：逐 tile 走 backbone_transform（通常在 CPU）\n",
    "    proc_list = []\n",
    "    for t in flat:\n",
    "        proc_list.append(backbone_transform(t.to(torch.float16)).unsqueeze(0))\n",
    "    proc = torch.cat(proc_list, dim=0).to(device=next(backbone.parameters()).device).to(dtype)\n",
    "\n",
    "    if USE_AUTOCAST_FOR_VIT and proc.is_cuda:\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=dtype):\n",
    "            feats = backbone(proc)  # [B*T, feat]\n",
    "    else:\n",
    "        feats = backbone(proc)  # [B*T, feat]\n",
    "    feats = feats.view(bsz, -1, feats.shape[-1])\n",
    "    return feats\n",
    "\n",
    "\n",
    "# 兼容旧实现签名：根据 VIT_PREPROCESS_MODE 选择 exact_timm 或 fast_gpu\n",
    "@torch.no_grad()\n",
    "def encode_tiles_infer(\n",
    "    images: torch.Tensor,\n",
    "    backbone: nn.Module,\n",
    "    backbone_transform,\n",
    "    grid: Tuple[int, int],\n",
    "    *,\n",
    "    dtype: torch.dtype,\n",
    ") -> torch.Tensor:\n",
    "    if VIT_PREPROCESS_MODE == \"exact_timm\":\n",
    "        return encode_tiles_exact_timm_transform(images, backbone, backbone_transform, grid, dtype=dtype)\n",
    "    return encode_tiles_fast(images, backbone, grid, dtype=dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Fast pipeline: ViT 预计算缓存 + head 推理（双进程/双卡并行）\n",
    "# =============================================================================\n",
    "def _read_test_unique_images(test_csv: Path) -> tuple[pd.DataFrame, List[str], List[str]]:\n",
    "    test_df = pd.read_csv(test_csv)\n",
    "    test_df[\"image_id\"] = test_df[\"sample_id\"].astype(str).str.split(\"__\", n=1).str[0]\n",
    "    img_df = test_df.drop_duplicates(\"image_id\")[[\"image_id\", \"image_path\"]].reset_index(drop=True)\n",
    "    return test_df, img_df[\"image_id\"].tolist(), img_df[\"image_path\"].tolist()\n",
    "\n",
    "\n",
    "def _split_indices(n: int, parts: int) -> List[np.ndarray]:\n",
    "    idx = np.arange(n)\n",
    "    return np.array_split(idx, parts)\n",
    "\n",
    "\n",
    "def _ensure_cache_memmaps(\n",
    "    *,\n",
    "    cache_dir: Path,\n",
    "    n: int,\n",
    "    small_tiles: int,\n",
    "    big_tiles: int,\n",
    "    feat_dim: int,\n",
    ") -> tuple[Path, Path, Path]:\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    small_path = cache_dir / \"tok_small.fp16.mmap\"\n",
    "    big_path = cache_dir / \"tok_big.fp16.mmap\"\n",
    "    meta_path = cache_dir / \"meta.json\"\n",
    "\n",
    "    # 创建/覆盖文件到目标大小（memmap 依赖文件大小）\n",
    "    def _init_file(p: Path, shape: tuple[int, ...], dtype: np.dtype):\n",
    "        mm = np.memmap(p, mode=\"w+\", dtype=dtype, shape=shape)\n",
    "        mm.flush()\n",
    "        del mm\n",
    "\n",
    "    np_dtype = np.float16 if CACHE_DTYPE == \"fp16\" else np.float32\n",
    "    _init_file(small_path, (n, small_tiles, feat_dim), np_dtype)\n",
    "    _init_file(big_path, (n, big_tiles, feat_dim), np_dtype)\n",
    "\n",
    "    meta = {\n",
    "        \"n\": n,\n",
    "        \"small_tiles\": small_tiles,\n",
    "        \"big_tiles\": big_tiles,\n",
    "        \"feat_dim\": feat_dim,\n",
    "        \"small_grid\": list(CFG.small_grid),\n",
    "        \"big_grid\": list(CFG.big_grid),\n",
    "        \"vit_feat_dim\": CFG.vit_feat_dim,\n",
    "    }\n",
    "    meta_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return small_path, big_path, meta_path\n",
    "\n",
    "\n",
    "def _open_cache_memmaps(\n",
    "    *,\n",
    "    small_path: Path,\n",
    "    big_path: Path,\n",
    "    n: int,\n",
    "    small_tiles: int,\n",
    "    big_tiles: int,\n",
    "    feat_dim: int,\n",
    "    mode: str,\n",
    ") -> tuple[np.memmap, np.memmap]:\n",
    "    np_dtype = np.float16 if CACHE_DTYPE == \"fp16\" else np.float32\n",
    "    tok_small = np.memmap(small_path, mode=mode, dtype=np_dtype, shape=(n, small_tiles, feat_dim))\n",
    "    tok_big = np.memmap(big_path, mode=mode, dtype=np_dtype, shape=(n, big_tiles, feat_dim))\n",
    "    return tok_small, tok_big\n",
    "\n",
    "\n",
    "def _load_image_tensor(image_root: Path, rel_path: str) -> torch.Tensor:\n",
    "    img_path = image_root / rel_path\n",
    "    img = cv2.imread(str(img_path))\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"image not found: {img_path}\")\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    x = torch.from_numpy(img).permute(2, 0, 1).contiguous()  # [3,H,W] uint8\n",
    "    x = x.to(torch.float32) / 255.0\n",
    "    return x\n",
    "\n",
    "\n",
    "def _vit_precompute_worker(\n",
    "    *,\n",
    "    device_id: int,\n",
    "    indices: np.ndarray,\n",
    "    image_paths: List[str],\n",
    "    image_root: Path,\n",
    "    small_path: Path,\n",
    "    big_path: Path,\n",
    "    n: int,\n",
    "    small_tiles: int,\n",
    "    big_tiles: int,\n",
    "    feat_dim: int,\n",
    "    vit_batch_size: int,\n",
    "    dtype_str: str,\n",
    ") -> None:\n",
    "    torch.cuda.set_device(device_id)\n",
    "    device = torch.device(f\"cuda:{device_id}\")\n",
    "    dtype = torch.float16 if dtype_str == \"fp16\" else torch.bfloat16\n",
    "\n",
    "    # ViT + transform\n",
    "    # - thread 后端：可选走“CPU 只加载一次 -> clone 到各 GPU”的共享模型，避免 CPU 同时存在两份 ViT\n",
    "    # - process 后端：每个进程各自加载（隔离更强）\n",
    "    if PARALLEL_BACKEND == \"thread\" and VIT_INIT_CPU_ONCE_CLONE_TO_GPU:\n",
    "        backbone, backbone_transform = _VIT_THREAD_MODELS[device_id]\n",
    "        # 确保 dtype/设备正确\n",
    "        backbone = backbone.to(device=device, dtype=dtype).eval()\n",
    "    else:\n",
    "        backbone, backbone_transform = build_vit_backbone(\n",
    "            vit_name=CFG.vit_name, vit_ckpt=CFG.vit_checkpoint, device=device, dtype=dtype\n",
    "        )\n",
    "\n",
    "    tok_small_mm, tok_big_mm = _open_cache_memmaps(\n",
    "        small_path=small_path,\n",
    "        big_path=big_path,\n",
    "        n=n,\n",
    "        small_tiles=small_tiles,\n",
    "        big_tiles=big_tiles,\n",
    "        feat_dim=feat_dim,\n",
    "        mode=\"r+\",\n",
    "    )\n",
    "\n",
    "    backbone.eval()\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, len(indices), vit_batch_size):\n",
    "            batch_idx = indices[start : start + vit_batch_size]\n",
    "            imgs = [_load_image_tensor(image_root, image_paths[i]) for i in batch_idx.tolist()]\n",
    "            x = torch.stack(imgs, dim=0).to(device=device, non_blocking=True)  # [B,3,H,W] float32\n",
    "            # 编码\n",
    "            # 恢复原版预处理路径（exact_timm）时，这里会走 timm transform；否则走 fast_gpu\n",
    "            feats_small = encode_tiles_infer(x, backbone, backbone_transform, CFG.small_grid, dtype=dtype)  # [B,Ts,4096]\n",
    "            feats_big = encode_tiles_infer(x, backbone, backbone_transform, CFG.big_grid, dtype=dtype)  # [B,Tb,4096]\n",
    "            if CACHE_DTYPE == \"fp32\":\n",
    "                tok_small_mm[batch_idx, :, :] = feats_small.detach().to(\"cpu\", torch.float32).numpy()\n",
    "                tok_big_mm[batch_idx, :, :] = feats_big.detach().to(\"cpu\", torch.float32).numpy()\n",
    "            else:\n",
    "                tok_small_mm[batch_idx, :, :] = feats_small.detach().to(\"cpu\", torch.float16).numpy()\n",
    "                tok_big_mm[batch_idx, :, :] = feats_big.detach().to(\"cpu\", torch.float16).numpy()\n",
    "            tok_small_mm.flush()\n",
    "            tok_big_mm.flush()\n",
    "\n",
    "    del tok_small_mm, tok_big_mm\n",
    "    # thread 后端下 backbone 是共享的，不在 worker 里 del\n",
    "    if not (PARALLEL_BACKEND == \"thread\" and VIT_INIT_CPU_ONCE_CLONE_TO_GPU):\n",
    "        del backbone\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def _load_fold_head_model(device: torch.device, dtype: torch.dtype, fold: int) -> CrossPVT_T2T_MambaHead:\n",
    "    head_cfg = HFConfig(\n",
    "        dropout=CFG.dropout,\n",
    "        hidden_ratio=CFG.hidden_ratio,\n",
    "        aux_head=CFG.aux_head,\n",
    "        small_grid=CFG.small_grid,\n",
    "        big_grid=CFG.big_grid,\n",
    "        pyramid_dims=CFG.pyramid_dims,\n",
    "        mamba_depth=CFG.mamba_depth,\n",
    "        mamba_kernel=CFG.mamba_kernel,\n",
    "        mobilevit_heads=CFG.mobilevit_heads,\n",
    "        mobilevit_depth=CFG.mobilevit_depth,\n",
    "        sra_heads=CFG.sra_heads,\n",
    "        sra_ratio=CFG.sra_ratio,\n",
    "        cross_heads=CFG.cross_heads,\n",
    "        cross_layers=CFG.cross_layers,\n",
    "        t2t_depth=CFG.t2t_depth,\n",
    "        vit_name=CFG.vit_name,\n",
    "        vit_checkpoint=Path(\"unused\"),\n",
    "        vit_feat_dim=CFG.vit_feat_dim,\n",
    "        token_embed_dim=CFG.token_embed_dim,\n",
    "    )\n",
    "    if HEAD_WEIGHT_DTYPE == \"fp32\":\n",
    "        head = CrossPVT_T2T_MambaHead(head_cfg).to(device=device, dtype=torch.float32).eval()\n",
    "    elif HEAD_WEIGHT_DTYPE == \"bf16\":\n",
    "        head = CrossPVT_T2T_MambaHead(head_cfg).to(device=device, dtype=torch.bfloat16).eval()\n",
    "    else:\n",
    "        head = CrossPVT_T2T_MambaHead(head_cfg).to(device=device, dtype=torch.float16).eval()\n",
    "\n",
    "    w = KAGGLE_HEAD_5FOLD_DIR / f\"fold{fold}\" / \"model.safetensors\"\n",
    "    if not w.exists():\n",
    "        raise FileNotFoundError(f\"找不到 fold{fold} 权重: {w}\")\n",
    "    sd = safetensors_load_file(str(w))\n",
    "    head_sd = {k[len(\"head.\") :]: v for k, v in sd.items() if k.startswith(\"head.\")}\n",
    "    missing, unexpected = head.load_state_dict(head_sd, strict=False)\n",
    "    if missing or unexpected:\n",
    "        raise RuntimeError(f\"fold{fold} head load_state_dict 不匹配：missing={missing[:10]} unexpected={unexpected[:10]}\")\n",
    "    return head\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _head_forward_to_logits(\n",
    "    head: CrossPVT_T2T_MambaHead,\n",
    "    tok_small: torch.Tensor,\n",
    "    tok_big: torch.Tensor,\n",
    "    small_grid: Tuple[int, int],\n",
    ") -> torch.Tensor:\n",
    "    if USE_AUTOCAST_FOR_HEAD and tok_small.is_cuda:\n",
    "        # Trainer(fp16/bf16) 的 eval 通常会启用 autocast；这里模拟同类行为以减小差异\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=tok_small.dtype):\n",
    "            out = head(tok_small, tok_big, small_grid, return_features=False)\n",
    "    else:\n",
    "        out = head(tok_small, tok_big, small_grid, return_features=False)\n",
    "    green = out[\"green\"]\n",
    "    clover = out[\"clover\"]\n",
    "    dead = out[\"dead\"]\n",
    "    gdm = green + clover\n",
    "    total = green + clover + dead\n",
    "    return torch.cat([green, dead, clover, gdm, total], dim=1)  # [B,5]\n",
    "\n",
    "\n",
    "def _head_infer_worker(\n",
    "    *,\n",
    "    device_id: int,\n",
    "    indices: np.ndarray,\n",
    "    small_path: Path,\n",
    "    big_path: Path,\n",
    "    pred_path: Path,\n",
    "    n: int,\n",
    "    small_tiles: int,\n",
    "    big_tiles: int,\n",
    "    feat_dim: int,\n",
    "    head_batch_size: int,\n",
    "    dtype_str: str,\n",
    ") -> None:\n",
    "    torch.cuda.set_device(device_id)\n",
    "    device = torch.device(f\"cuda:{device_id}\")\n",
    "    dtype = torch.float16 if dtype_str == \"fp16\" else torch.bfloat16\n",
    "\n",
    "    tok_small_mm, tok_big_mm = _open_cache_memmaps(\n",
    "        small_path=small_path,\n",
    "        big_path=big_path,\n",
    "        n=n,\n",
    "        small_tiles=small_tiles,\n",
    "        big_tiles=big_tiles,\n",
    "        feat_dim=feat_dim,\n",
    "        mode=\"r\",\n",
    "    )\n",
    "    pred_mm = np.memmap(pred_path, mode=\"r+\", dtype=np.float32, shape=(n, 5))\n",
    "\n",
    "    # 每张卡各加载 5 个 head（你要求的形式）；数据按 indices 分片做 data-parallel\n",
    "    heads = [_load_fold_head_model(device, dtype, fold) for fold in FOLDS]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, len(indices), head_batch_size):\n",
    "            batch_idx = indices[start : start + head_batch_size]\n",
    "            # 缓存读取：fp32 更贴近原始特征；之后再转成 head 需要的 dtype\n",
    "            if CACHE_DTYPE == \"fp32\":\n",
    "                small_np = np.asarray(tok_small_mm[batch_idx, :, :], dtype=np.float32)\n",
    "                big_np = np.asarray(tok_big_mm[batch_idx, :, :], dtype=np.float32)\n",
    "                tok_small = torch.from_numpy(small_np).to(device=device, dtype=dtype, non_blocking=True)\n",
    "                tok_big = torch.from_numpy(big_np).to(device=device, dtype=dtype, non_blocking=True)\n",
    "            else:\n",
    "                small_np = np.asarray(tok_small_mm[batch_idx, :, :], dtype=np.float16)\n",
    "                big_np = np.asarray(tok_big_mm[batch_idx, :, :], dtype=np.float16)\n",
    "                tok_small = torch.from_numpy(small_np).to(device=device, dtype=dtype, non_blocking=True)\n",
    "                tok_big = torch.from_numpy(big_np).to(device=device, dtype=dtype, non_blocking=True)\n",
    "\n",
    "            logits_sum = None\n",
    "            for head in heads:\n",
    "                logits = _head_forward_to_logits(head, tok_small, tok_big, CFG.small_grid)\n",
    "                logits_sum = logits if logits_sum is None else (logits_sum + logits)\n",
    "            logits_mean = logits_sum / float(len(heads))\n",
    "            pred_mm[batch_idx, :] = logits_mean.detach().to(\"cpu\", torch.float32).numpy()\n",
    "            pred_mm.flush()\n",
    "\n",
    "    del tok_small_mm, tok_big_mm, pred_mm\n",
    "    del heads\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def run_predict_5fold_fast_cached() -> Path:\n",
    "    if not torch.cuda.is_available() or torch.cuda.device_count() < 2:\n",
    "        raise RuntimeError(\"FAST_INFER 需要至少 2 张 GPU（当前不可用）\")\n",
    "\n",
    "    test_df, image_ids, image_paths = _read_test_unique_images(TEST_CSV)\n",
    "    n = len(image_ids)\n",
    "    small_tiles = CFG.small_grid[0] * CFG.small_grid[1]\n",
    "    big_tiles = CFG.big_grid[0] * CFG.big_grid[1]\n",
    "    feat_dim = CFG.vit_feat_dim\n",
    "\n",
    "    CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    (CACHE_DIR / \"image_ids.txt\").write_text(\"\\n\".join(image_ids), encoding=\"utf-8\")\n",
    "\n",
    "    small_path, big_path, _ = _ensure_cache_memmaps(\n",
    "        cache_dir=CACHE_DIR, n=n, small_tiles=small_tiles, big_tiles=big_tiles, feat_dim=feat_dim\n",
    "    )\n",
    "\n",
    "    # dtype 选择：ViT/head 都用同一个（通常 fp16 更通用）\n",
    "    dtype = pick_infer_dtype()\n",
    "    dtype_str = \"bf16\" if dtype == torch.bfloat16 else \"fp16\"\n",
    "\n",
    "    # 1) ViT 预计算（双进程/双卡）\n",
    "    idx_splits = _split_indices(n, parts=2)\n",
    "    backend = PARALLEL_BACKEND\n",
    "    if backend == \"process\" and _in_notebook():\n",
    "        print(\"[WARN] Notebook 环境下 process/spawn 容易 pickle 失败，已自动切换为 thread 后端。\")\n",
    "        backend = \"thread\"\n",
    "\n",
    "    # thread 后端下：先 CPU 加载一次 ViT，再 clone 到两张 GPU，避免 CPU 同时存在两份 ViT\n",
    "    if backend == \"thread\" and VIT_INIT_CPU_ONCE_CLONE_TO_GPU:\n",
    "        _init_vit_models_for_thread_backend(GPU_IDS, dtype=dtype)\n",
    "\n",
    "    vit_jobs = []\n",
    "    for proc_i, dev in enumerate(GPU_IDS):\n",
    "        vit_jobs.append(\n",
    "            (\n",
    "                _vit_precompute_worker,\n",
    "                dict(\n",
    "                    device_id=dev,\n",
    "                    indices=idx_splits[proc_i],\n",
    "                    image_paths=image_paths,\n",
    "                    image_root=IMAGE_ROOT,\n",
    "                    small_path=small_path,\n",
    "                    big_path=big_path,\n",
    "                    n=n,\n",
    "                    small_tiles=small_tiles,\n",
    "                    big_tiles=big_tiles,\n",
    "                    feat_dim=feat_dim,\n",
    "                    vit_batch_size=VIT_BATCH_SIZE,\n",
    "                    dtype_str=dtype_str,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    _run_parallel_jobs(vit_jobs, backend=backend)\n",
    "\n",
    "    # 2) head 推理（双进程/双卡），每卡各加载 5 个 head，对数据分片\n",
    "    pred_path = CACHE_DIR / \"pred.fp32.mmap\"\n",
    "    pred_mm = np.memmap(pred_path, mode=\"w+\", dtype=np.float32, shape=(n, 5))\n",
    "    pred_mm.flush()\n",
    "    del pred_mm\n",
    "\n",
    "    head_jobs = []\n",
    "    for proc_i, dev in enumerate(GPU_IDS):\n",
    "        head_jobs.append(\n",
    "            (\n",
    "                _head_infer_worker,\n",
    "                dict(\n",
    "                    device_id=dev,\n",
    "                    indices=idx_splits[proc_i],\n",
    "                    small_path=small_path,\n",
    "                    big_path=big_path,\n",
    "                    pred_path=pred_path,\n",
    "                    n=n,\n",
    "                    small_tiles=small_tiles,\n",
    "                    big_tiles=big_tiles,\n",
    "                    feat_dim=feat_dim,\n",
    "                    head_batch_size=HEAD_BATCH_SIZE,\n",
    "                    dtype_str=dtype_str,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    _run_parallel_jobs(head_jobs, backend=backend)\n",
    "\n",
    "    # 3) 聚合写 submission\n",
    "    pred_mm = np.memmap(pred_path, mode=\"r\", dtype=np.float32, shape=(n, 5))\n",
    "    pred_5 = np.asarray(pred_mm, dtype=np.float32)\n",
    "    del pred_mm\n",
    "\n",
    "    # image_id -> row index\n",
    "    id2idx = {iid: i for i, iid in enumerate(image_ids)}\n",
    "    col2j = {c: j for j, c in enumerate(TARGET_COLS)}\n",
    "    idx_arr = test_df[\"image_id\"].map(id2idx).to_numpy()\n",
    "    col_arr = test_df[\"target_name\"].map(col2j).to_numpy()\n",
    "    y = pred_5[idx_arr, col_arr].astype(np.float32)\n",
    "\n",
    "    sub = pd.DataFrame({\"sample_id\": test_df[\"sample_id\"].values, \"target\": y})\n",
    "    out_csv = OUTPUT_DIR / \"submission.csv\"\n",
    "    sub.to_csv(out_csv, index=False)\n",
    "    return out_csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved: kaggle_out/submission.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Kaggle 只需要跑这句就能生成 submission.csv（FAST 双卡缓存推理）\n",
    "    out = run_predict_5fold_fast_cached()\n",
    "    print(f\"[OK] saved: {out}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csrio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
